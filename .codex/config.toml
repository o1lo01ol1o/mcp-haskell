model = "gpt-5.2"
model_reasoning_effort = "high"
[mcp_servers.ghcid]
command = "direnv"
args = ["exec", ".", "./scripts/run-mcp-ghcid.sh", "--log-level", "warn"]
startup_timeout_sec = 30
[mcp_servers.ghcid.env]
MCP_AUTO_BUILD = "0"
MCP_LAUNCHER_DEBUG = "0"
# Codex runs MCP servers under a filesystem sandbox by default; force Cabal to
# use the repo-local `.cabal/` (gitignored) instead of `~/.cabal`.
CABAL_DIR = ".cabal"

[mcp_servers.hls]
command = "direnv"
args = ["exec", ".", "./scripts/run-mcp-hls.sh", "--log-level", "debug"]
startup_timeout_sec = 30
enabled = false
[mcp_servers.hls.env]
MCP_AUTO_BUILD = "0"
MCP_LAUNCHER_DEBUG = "0"
# Keep HLS/Cabal state inside the repo when enabled.
CABAL_DIR = ".cabal"

# OpenAI via Responses API (used for GPT-5)
[model_providers.openai]
name = "OpenAI"
base_url = "https://api.openai.com/v1"
env_key = "OPENAI_API_KEY"
request_max_retries = 40
stream_max_retries = 100
stream_idle_timeout_ms = 300000
wire_api = "responses"

[projects."/Users/timpierson/Work/mcp-hls"]
trust_level = "trusted"

[notice]
hide_gpt5_1_migration_prompt = true
